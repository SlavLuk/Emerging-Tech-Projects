{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-written digits Recognition Notebook\n",
    "\n",
    "This notebook is intended to demonstrate the implementation of a Convolutional neural network model in Keras (backend TensorFlow) for recognising handwritten digits in images. The original paper could be found [here](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf). Before getting started, import any libraries that may be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "\n",
    "#This module performs conversions between Python values and C structs represented as Python bytes objects.\n",
    "import struct as st \n",
    "# Efficiently deals with numerical multi-dimensional arrays.\n",
    "import numpy as np \n",
    "# Matplotlib is a plotting library. pyplot is its easy-to-use module.\n",
    "import matplotlib.pyplot as plt\n",
    "# Tensorflow backend keras\n",
    "import tensorflow.keras as kr\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First method to read and convert mnist dataset in idx format into python numpy array\n",
    "\n",
    "The next step is to load the data to train and test the model.The MNIST database of handwritten digits, available from [this page](http://yann.lecun.com/exdb/mnist/), has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Adapted from : https://medium.com/@mannasiladittya/converting-mnist-data-in-idx-format-to-python-numpy-array-5cb9126f99f1\n",
    "\n",
    "\n",
    "filename = {'train_images' : './mnist_dataset/train-images.idx3-ubyte' ,'train_labels' : './mnist_dataset/train-labels.idx1-ubyte',\n",
    "            'test_img':'./mnist_dataset/t10k-images.idx3-ubyte','test_lbl':'./mnist_dataset/t10k-labels.idx1-ubyte'}\n",
    "# Open the IDX file in readable binary mode.\n",
    "train_imagesfile = open(filename['train_images'],'rb')\n",
    "train_labelsfile = open(filename['train_labels'],'rb')\n",
    "test_imagesfile = open(filename['test_img'],'rb')\n",
    "test_labelsfile = open(filename['test_lbl'],'rb')\n",
    "\n",
    "# Set pointer to the beginning of the file.\n",
    "train_imagesfile.seek(0)\n",
    "train_labelsfile.seek(0)\n",
    "test_imagesfile.seek(0)\n",
    "test_labelsfile.seek(0)\n",
    "\n",
    "# Read the magic number\n",
    "magic_img = st.unpack('>4B',train_imagesfile.read(4))\n",
    "magic_lab = st.unpack('>4B',train_labelsfile.read(4))\n",
    "magic_test_img = st.unpack('>4B',test_imagesfile.read(4))\n",
    "magic_test_lab = st.unpack('>4B',test_labelsfile.read(4))\n",
    "\n",
    "# Read the dimensions of the Image data-set\n",
    "train_images = st.unpack('>I',train_imagesfile.read(4))[0] #num of images\n",
    "n_row_i = st.unpack('>I',train_imagesfile.read(4))[0] #num of rows\n",
    "n_col_i = st.unpack('>I',train_imagesfile.read(4))[0] #num of column\n",
    "\n",
    "test_images = st.unpack('>I',test_imagesfile.read(4))[0] #num of images\n",
    "n_row_test = st.unpack('>I',test_imagesfile.read(4))[0] #num of rows\n",
    "n_col_test = st.unpack('>I',test_imagesfile.read(4))[0] #num of column\n",
    "\n",
    "# Read the dimensions of the Label data-set\n",
    "train_labels = st.unpack('>I',train_labelsfile.read(4))[0] #num of items\n",
    "test_labels = st.unpack('>I',test_labelsfile.read(4))[0] #num of items\n",
    "\n",
    "# Reading the Image data\n",
    "train_bytes_total = train_images*n_row_i*n_col_i*1 \n",
    "test_bytes_total = test_images*n_row_test*n_col_test*1 \n",
    "\n",
    "# 'B' is used since it is of 'unsigned char' C type and ‘integer’ Python type\n",
    "# and has standard size 1 as mentioned in the official documentation of struct.\n",
    "# ‘>’ is used since the data is in MSB first (high endian) format used by most \n",
    "# non-Intel processors, as mentioned in their original website.\n",
    "train_img = 255 - np.asarray(st.unpack('>'+'B'*train_bytes_total,train_imagesfile.read(train_bytes_total))).reshape((train_images,n_row_i,n_col_i))\n",
    "test_img =   255 - np.asarray(st.unpack('>'+'B'*test_bytes_total,test_imagesfile.read(test_bytes_total))).reshape((test_images,n_row_test,n_col_test))\n",
    " \n",
    "# Reading the label data\n",
    "train_lbl = np.asarray(st.unpack('>'+'B'*train_labels,train_labelsfile.read(train_labels))).reshape((train_labels))\n",
    "test_lbl = np.asarray(st.unpack('>'+'B'*test_labels,test_labelsfile.read(test_labels))).reshape((test_labels))\n",
    "\n",
    "# Close read files\n",
    "train_imagesfile.close()\n",
    "train_labelsfile.close()\n",
    "test_imagesfile.close()\n",
    "test_labelsfile.close()\n",
    "\n",
    "print(test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_img[132], cmap='gray')\n",
    "\n",
    "print(test_lbl[132])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second method to load the MNIST dataset into Python numpy array\n",
    "\n",
    "In order to load the data to train and test the model. For this we will use the MNIST data set available from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Train set of images are 3D array (60000, 28, 28)\n",
      "Test set of images are 3D array (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# import MNIST dataset with keras\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(type(x_train))\n",
    "print('Train set of images are 3D array',x_train.shape)# 60000 of images in train set,sized by 28x28 pixals\n",
    "print('Test set of images are 3D array',x_test.shape)# 10000 of images in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping data\n",
    "\n",
    "The datasets are 3D arrays. Training dataset shape is (60000, 28, 28) & Testing dataset shape is (10000, 28, 28). The input shape that CNN expects is a 4D array (batch, height, width, channels). Channels signify whether the image is grayscale or colored. In our case, we are using grayscale images so we give 1 for channels if these are colored images we give 3(RGB). Below code for reshaping our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape to the expected CNN format \n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "print(x_train.shape)# 60000 images,28x28x1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "- When our data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale.\n",
    "- This is useful for optimization algorithmsm used in the core of machine learning algorithms like gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# scaling images to make gradient descent process faster\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "print(x_train[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Values in Python\n",
    "\n",
    "Categorical data are variables that contain label values rather than numeric values.\n",
    "Many machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric.\n",
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# encoding categorical variables (one,two,three...) into numerical (1,2,3...)\n",
    "# adapted from : https://pbpython.com/categorical-encoding.html\n",
    "\n",
    "y_train = kr.utils.to_categorical(y_train, 10)\n",
    "y_test = kr.utils.to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2be22b03fc8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(' 0, 1, 2, 3, 4, 5, 6, 7, 8, 9')\n",
    "print(y_test[132])\n",
    "plt.imshow(x_test[132].reshape(28, 28), cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "A Convolutional Neural Network (CNN) is comprised of one or more convolutional layers (often with a subsampling step) and then followed by one or more fully connected layers as in a standard multilayer neural network. The architecture of a CNN is designed to take advantage of the 2D structure of an input image (or other 2D input such as a speech signal). This is achieved with local connections and tied weights followed by some form of pooling which results in translation invariant features. Another benefit of CNNs is that they are easier to train and have many fewer parameters than fully connected networks with the same number of hidden units.\n",
    "\n",
    "A CNN consists of a number of convolutional and subsampling layers optionally followed by fully connected layers. The input to a convolutional layer is a m x m x r image where m is the height and width of the image and r is the number of channels, e.g. an RGB image has r=3,in our case we have only one channel thus r = 1.\n",
    "\n",
    "The convolutional layer will have k filters (or kernels) of size n x n x q where n is smaller than the dimension of the image and q can either be the same as the number of channels r or smaller and may vary for each kernel. The size of the filters gives rise to the locally connected structure which are each convolved with the image to produce k feature maps of size m−n+1. Each map is then subsampled typically with mean or max pooling over p x p contiguous regions where p ranges between 2 for small images (e.g. MNIST) and is usually not more than 5 for larger inputs. Either before or after the subsampling layer an additive bias and sigmoidal nonlinearity is applied to each feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Convolutional neural network](./img/conv.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Convolutional Neural Network\n",
    "\n",
    "Steps to build CNN\n",
    "1. Provide the input image into convolution layer.\n",
    "2. Take convolution with featured kernel/filters.\n",
    "3. Apply pooling layer to reduce the dimensions.\n",
    "4. Add these layers multiple times.\n",
    "5. Flatten the output and feed into a fully connected layer.\n",
    "6. Train the model with backpropagation using logistic regression.\n",
    "\n",
    "    * The first layer of code is a hidden layer called a Convolution2D. The layer has 32 filters/output channels, which with the size of 3×3 and an activation function. This is also the input layer, expecting images with the structure outlined above (height, width, channels).(28x28x1)\n",
    "    * The Second layer is the MaxPooling layer. MaxPooling layer is used to down-sample the input to enable the model to make assumptions about the features so as to reduce over-fitting. It also reduces the number of parameters to learn, reducing the training time.\n",
    "    * The third layer is a hidden layer Convolution2D with 64 filters/output channels with the size of 3×3 and an activation function.Because it is not the first layer there is no need for input_shape parameters.\n",
    "    * The forth layer is a MaxPooling layer.\n",
    "    * The sixth layer converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by a fully connected neural network.\n",
    "    * The seventh is a fully connected layer with 128 neurons.\n",
    "    * The eighth and final layer is an output layer with 10 neurons and it uses softmax activation function. Each neuron will give the probability of that class. It’s a multi-class classification that’s why softmax activation function if it was a binary classification we would use sigmoid activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "model = Sequential()\n",
    "#  Implements the forward propagation for the model:\n",
    "#   CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "\n",
    "# Convolution layer\n",
    "model.add(Conv2D(32,(3,3),strides=(1, 1),input_shape = (x_train.shape[1],x_train.shape[2],1),activation = 'relu',padding='same'))\n",
    "\n",
    "# Pooling as reducing feature map\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "# Apply 64 filters sized of (3x3) on 2nd convolution layer\n",
    "model.add(Conv2D(64,kernel_size=(3,3),strides=(1, 1),activation = 'relu',padding='same')) \n",
    "          \n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# Full connection\n",
    "model.add(Dense(model.output_shape[1],activation = 'relu'))\n",
    "model.add(Dense(10,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Model\n",
    "\n",
    "To complile the model I used categorical_crossentropy as a loss function because its a multi-class classification problem.\n",
    "Categorical crossentropy is a loss function that is used for single label categorization. This is when only one category is applicable for each data point. In other words, an example can belong to one class only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compiling of the Model\n",
    "model.compile( optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model for a fixed number of epochs (iterations on a dataset).\n",
    "\n",
    "The test data is used as the validation dataset, allowing you to see the skill of the model as it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 305s 5ms/sample - loss: 0.1833 - accuracy: 0.9441 - val_loss: 0.0412 - val_accuracy: 0.9867\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 299s 5ms/sample - loss: 0.0607 - accuracy: 0.9812 - val_loss: 0.0342 - val_accuracy: 0.9893\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 295s 5ms/sample - loss: 0.0458 - accuracy: 0.9853 - val_loss: 0.0245 - val_accuracy: 0.9913\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 294s 5ms/sample - loss: 0.0326 - accuracy: 0.9893 - val_loss: 0.0221 - val_accuracy: 0.9926\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 293s 5ms/sample - loss: 0.0282 - accuracy: 0.9911 - val_loss: 0.0211 - val_accuracy: 0.9925\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 297s 5ms/sample - loss: 0.0251 - accuracy: 0.9912 - val_loss: 0.0252 - val_accuracy: 0.9919\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 302s 5ms/sample - loss: 0.0212 - accuracy: 0.9929 - val_loss: 0.0207 - val_accuracy: 0.9931\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 305s 5ms/sample - loss: 0.0187 - accuracy: 0.9940 - val_loss: 0.0206 - val_accuracy: 0.9935\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 351s 6ms/sample - loss: 0.0169 - accuracy: 0.9943 - val_loss: 0.0202 - val_accuracy: 0.9926\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 325s 5ms/sample - loss: 0.0153 - accuracy: 0.9948 - val_loss: 0.0227 - val_accuracy: 0.9919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be2469e948>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the Model\n",
    "The test dataset is used to evaluate the model and after evaluation Test loss & Test Accuracy metrics will be printed. I achieved a 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics(Test loss & Test Accuracy): \n",
      "[0.022652018182754547, 0.9919]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation of the model\n",
    "metrics = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Metrics(Test loss & Test Accuracy): \")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the  Model\n",
    "\n",
    "I am saving this model for the future use in my web application to predict user's hand written digits.\n",
    "\n",
    "To save a Keras model into a single HDF5 file which will contain:\n",
    "- the architecture of the model, allowing to re-create the model\n",
    "- the weights of the model\n",
    "- the training configuration (loss, optimizer)\n",
    "- the state of the optimizer, allowing to resume training exactly where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save everything to h5 file format\n",
    "model.save('./model/model.h5')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-creating saved model\n",
    "\n",
    "To reinstantiate our model we can use __load_model__ .It will also take care of compiling the model using the saved training configuration (unless the model was never compiled in the first place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load already compiled model\n",
    "load_model = load_model('./model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------  0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "Actual label :  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Predicted class :  [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prd_img = load_model.predict_classes(x_test[50].reshape(-1,28,28,1))\n",
    "\n",
    "plt.imshow(x_test[50].reshape(28, 28), cmap='gray')\n",
    "print('---------------  0, 1, 2, 3, 4, 5, 6, 7, 8, 9',)\n",
    "print('Actual label : ', y_test[50])\n",
    "print('Predicted class : ',prd_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Ian McLoughlin Lecture notes and Videos\n",
    "- [Converting MNIST dataset for Handwritten digit recognition.](https://medium.com/@mannasiladittya/converting-mnist-data-in-idx-format-to-python-numpy-array-5cb9126f99f1)\n",
    "- [An intuitive guide to Convolutional Neural Networks.](https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050)\n",
    "- [Understanding of Convolutional Neural Network (CNN).](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148)\n",
    "- [Keras Convolutional Layers](https://keras.io/layers/convolutional/)\n",
    "- [Data Preprocessing for Machine learning in Python](https://www.geeksforgeeks.org/data-preprocessing-machine-learning-python/)\n",
    "- [Encoding Categorical Values in Pythonm](https://pbpython.com/categorical-encoding.html)\n",
    "- [Convolutional Neural Network](http://deeplearning.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/)\n",
    "- [Loss functions](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
